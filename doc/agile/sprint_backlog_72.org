#+title: Sprint Backlog 72
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) spike(p) }

* Mission Statement

- Perform the main refactors to yarn and cpp;

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75
#+CAPTION: Clock summary at [2015-08-18 Tue 17:42]
| <75>                                                                        |         |       |      |
| Headline                                                                    | Time    |       |      |
|-----------------------------------------------------------------------------+---------+-------+------|
| *Total time*                                                                | *13:11* |       |      |
|-----------------------------------------------------------------------------+---------+-------+------|
| Stories                                                                     | 13:11   |       |      |
| Active                                                                      |         | 13:11 |      |
| STARTED Sprint and product backlog grooming                                 |         |       | 0:24 |
| STARTED Refactor qname                                                      |         |       | 9:48 |
| STARTED Refactor code around model origination                              |         |       | 0:08 |
| STARTED Remove primitive model handling in tack dia transformer             |         |       | 0:13 |
| STARTED Tack refactor around partial model construction                     |         |       | 2:38 |
#+end:

*** STARTED Sprint and product backlog grooming                       :story:
    CLOCK: [2015-08-10 Mon 15:04]--[2015-08-10 Mon 15:10] =>  0:06
    CLOCK: [2015-08-10 Mon 14:45]--[2015-08-10 Mon 15:03] =>  0:18

Updates to sprint and product backlog.

*** COMPLETED Rename =tack= to =yarn=                                 :story:
    CLOSED: [2015-10-15 Thu 20:28]
    CLOCK: [2015-10-15 Thu 19:21]--[2015-10-15 Thu 20:28] =>  1:07
    CLOCK: [2015-10-15 Thu 07:51]--[2015-10-15 Thu 08:30] =>  0:39

We were a bit too trigger happy by settling on =tack= as the name for
the "temporary" meta-model. As it turns out, we don't need two
meta-models and as such the name =tack= is no longer suitable. The
right thing to do is to rename the model to =yarn=.

*** STARTED Refactor qname                                            :story:
    CLOCK: [2015-08-20 Thu 07:52]--[2015-08-20 Thu 08:36] =>  0:44
    CLOCK: [2015-08-18 Tue 14:24]--[2015-08-18 Tue 14:39] =>  0:15
    CLOCK: [2015-08-15 Sat 13:45]--[2015-08-15 Sat 14:38] =>  0:53
    CLOCK: [2015-08-15 Sat 12:01]--[2015-08-15 Sat 12:36] =>  0:35
    CLOCK: [2015-08-14 Fri 20:34]--[2015-08-14 Fri 21:20] =>  0:46
    CLOCK: [2015-08-14 Fri 19:20]--[2015-08-14 Fri 20:33] =>  1:13
    CLOCK: [2015-08-14 Fri 14:51]--[2015-08-14 Fri 16:55] =>  2:04
    CLOCK: [2015-08-13 Thu 08:02]--[2015-08-13 Thu 08:48] =>  0:46
    CLOCK: [2015-08-10 Mon 16:51]--[2015-08-10 Mon 18:30] =>  1:39
    CLOCK: [2015-08-10 Mon 16:46]--[2015-08-10 Mon 16:50] =>  0:04
    CLOCK: [2015-08-10 Mon 16:43]--[2015-08-10 Mon 16:45] =>  0:02
    CLOCK: [2015-08-10 Mon 15:11]--[2015-08-10 Mon 16:42] =>  1:31

Split qname into name and location; location is made up of model name,
external module path, model path, internal module path.

Notes:

- populate model path as module name by default unless supplied by
  field.
- deal with the fallout in terms of file paths creation, etc.
- fix hardware model to supply model name but to have a blank model
  path.
- split model names with dots into multiple model paths.
- do not populate model path and qualified until resolution is done -
  these properties do not add any value. After resolution - perhaps
  as a last pass of the resolver - go through every single qname and
  compute these properties. This means that all calls to qualified
  prior to this need to be replaced to direct calls to qualified name
  builder.

More notes:

- within a partial model, there are two stages of processing: an
  initial pass in which we can identify all of the names of the
  elements declared in a model; and a second pass in which we can
  resolve all properties that belong to that model. By "resolve" we
  mean we can figure out if a property is referring to an element in a
  module inside the model or if its referring to an element in a
  different model. This can only be done when we have all the names of
  all the modules in the model.
- there is such a thing as a location: an object which allows one to
  figure out where a type is located in an imaginary "element
  space". In addition to the location, the element space has another
  dimension, given by the element "simple" name (from now on just
  name). The pair =(location, name)= corresponds to a unique point in
  the element space.
- there is such a thing as a unique element identifier: it is a string
  representation of the pair =(location, name)= according to a
  well-defined syntax.
- the pair =(location, name)= is an element identifier, because it
  uniquely identifies elements in the element space.
- the external module path is required to allow us to represent
  external containment; that is, cases where the model is contained in
  one or more namespaces, but we do not want to represent these inside
  the model.
- the internal module path is required to allow us to represent
  internal containment; that is, the element is contained in one or
  more modules, represented in the model.
- the model path represents containment inferred from the model name
  itself; that is, a composite model name such as =a.b.c=.
- the model name does not always contribute to the model path. For
  models such as hardware, the model has to have a name (it cannot be
  in a nameless file) but the types are in the global space. This
  means that we need to switch on/off the ability to have the model
  name contribute to the model namespace.
- model names are only relevant initially. We could store them in
  model class, but they will be thrown away during merging.
- references are used for several purposes: a) to determine that we
  have loaded all required models. b) to generate code dependencies
  against dependent models: at present just linking and registrar in
  serialisation. In order to figure out what to do with the reference
  we need to know its "kind". For dogen models, we need to generate
  registrars; for non-dogen models we do not. We always need to
  link. At present this is done via the origin types property. A
  better way of modeling this may be "is dogen model" or something
  along these lines.
- one model may have more than one set of link instructions. These are
  more related to the types than with the model itself. For example,
  in boost we need to link potentially against multiple
  libraries. This could be modeled by a dynamic property at the type
  level or model level. For dogen models it would be model level. The
  property may be empty (hardware, std).
- from a element identifier it is not possible to determine its model
  name. It may or may not be reconstructible from the model
  path. However, if one were to have a map of location to model name,
  one could at least figure out if the type is on any of the loaded
  models. We could keep track of all locations which are not within
  the model. Those must match the referenced models or else there is a
  type resolution failure.
- there is such a thing as a element instance identifier. We call it
  nested name at present. The element instance identifier identifies
  instantiations of types. It models two cases: for the case where the
  type has no type parameters, the instance identifier is equal to the
  element identifier; for all other cases, it is a hierarchical
  collection of element identifiers, modeling the type parameter
  structure.
- a model should have: an element identifier which is identical to the
  root module (the module that represents the model). A model is
  itself an element.

 a location; a name (meaning the original,
  possibly composite, model name); a

the
  types pace is hierarchical: its made up of the global namespace at
  the top (where types in the hardware model live), and then followed
  by all other namespaces "declared" at the top-level.
- there are four distinct cases of locations in the type space

Merged stories:

*Consider renaming qname*

As part of dynamic we came up with a better way of modeling names:
type is name, fields:

- simple
- qualified

This is a better way of modeling, as opposed to the SML way with a
=qname= which then contains a =simple_name=. We should use this
approach in SML to.

*Split model name from "contributing model name" in qname*

We need to find a way to model qnames such that there are two model
names: one which contributes to the namespaces and another which
doesn't. The specific use case is the primitives model where the model
has to have a name but we don't want the type names to have the model
name. Perhaps we need some kind of flag: model name contributes to
namespacing.

With this we can then remove the numerous hacks around the primitives
model name such as:

- // FIXME: mega hack to handle primitive model.

See comment in 'dot' story - we can have a model name and a model
package.

*** STARTED Refactor code around model origination                    :story:
    CLOCK: [2015-08-18 Tue 14:39]--[2015-08-18 Tue 14:47] =>  0:08

- remove origin types and generation types, replacing it with just a
  boolean for is target.
- at present we are using origin type to determine whether to create a
  registrar, etc in cpp model. There is no other use case for
  this. This is done in several places due to the bad handling of C++
  specific types. Grep for =references= in =cpp= to find all
  locations.

*Previous Understanding*

In the past we added a number of knobs around generation, all with
their own problems:

- =origin_types=: was the model/type created by the user or the
  system. in reality this means did the model come from Dia or
  JSON. this is confusing as the user can also add JSON files (their
  own model library) and in the future the user can use JSON
  exclusively without needed Dia at all.

- =generation_types=: if the model is target, all types are to be
  generated /unless/ they are not properly supported, in which case
  they are to be "partially" generated (as is the case with
  services). This is a formatter decision and SML should not know
  anything about it.

These can be replaced by a single enumeration that indicates if the
type/model is target or not.

This work should be integrated with the model types story.

*** STARTED Remove primitive model handling in tack dia transformer   :story:
    CLOCK: [2015-08-18 Tue 14:48]--[2015-08-18 Tue 15:01] =>  0:13

We seem to be doing some handling for primitives which is no longer
required. The handling of current model is also very dodgy. All in
transformer's update model reference.

Actually this is nothing at all to do with the primitive model but all
to do with computing the correct name. We need to start using the
builder here.

*** STARTED Tack refactor around partial model construction           :story:
    CLOCK: [2015-08-18 Tue 16:55]--[2015-08-18 Tue 17:41] =>  0:46
    CLOCK: [2015-08-18 Tue 16:32]--[2015-08-18 Tue 16:54] =>  0:22
    CLOCK: [2015-08-18 Tue 15:02]--[2015-08-18 Tue 16:32] =>  1:30

There are a number of activities done in the file importers which
really belong to the main meta-model. We should create a single
workflow for "post-processing" with these activities and move them
away from the importers.

Notes:

- add a module post processor that computes owner (containing
  module?), members, is top level. Seems like we already have a
  top-level module: containing module is null.
- add unparsed name to nested name. Update importers to read the
  unparsed name and not expand it. Create a "property expander" that
  parses the unparsed name and expands it to a proper nested
  name. Compute qualified for property types.
- reference expander to compute references.
- add some enumeration post-processing that assigns it a underlying
  type. Should be done with merged model (look for a primitive type with
  property =is_default_enumeration_type=).

*** Improve references management                                     :story:

At present, we compute model references as follows:

- in dia to sml we first loop through all types and figure out the
  distinct model names. This is done by creating a "shallow" qname
  with just the model name and setting its origin type to unknown.
- when we merge, we take the references of target - the only ones we
  care about - and then we check that against the list of the models
  we are about to merge. If there are any missing models we complain
  (see comments below). We then loop through the list of references
  and "resolve" the origin type of the model.

Note: We could actually also complain if there are too many models, or
more cleverly avoid merging those models which are not required. Or
even more cleverly, we could avoid loading them in the first place, if
only we could load target first.

A slightly better way of doing this would be:

- in SML create a references updater that takes a model and computes
  its reference requirements. It could also receive a list of "other"
  models from which to get their origin types to avoid using =unknown=
  at all, and checks that all reference requirements have been met.
- the current step =update_references= is just a call to the
  references updater, prior to merging, with the target model.

Note:

It seems that the references are incorrect at present; on rebuild, we
see serialisation's registrar moving for no reason:

: -    dogen::config::register_types(ar);
:      dogen::sml::register_types(ar);
: -    dogen::dynamic::schema::register_types(ar);
: +    dogen::config::register_types(ar);

The references have not changed at all in the dogen invocation:

:    --reference ${CMAKE_SOURCE_DIR}/diagrams/config.dia,dogen
:    --reference ${CMAKE_SOURCE_DIR}/diagrams/sml.dia,dogen
:    --reference ${CMAKE_SOURCE_DIR}/diagrams/formatters.dia,dogen
:    --reference ${CMAKE_SOURCE_DIR}/diagrams/schema.dia,dogen::dynamic

We need to fix this with the refactor.

*** Add =operator<= for names                                         :story:

We seem to redefine this all over the place. Create a utility class
somewhere.

*** Services and leaves are not properly handled                      :story:

We are manually ignoring services when calculating leaves.

*** Add support for model names with dots                             :story:

It is quite annoying to have to create folders and sub-folders for the
main projects. This is not too bad right now because we don't really
make use of nesting that much, other than with test models. However,
now that the architecture is clear and we need to make use of nesting,
it becomes more of a concern. For example:

: / a
:   / b
:   / c
: / d
:   / e
:   / f

This is clearer as:

: / a
: / a.b
: / a.c
: / d.e
: / d.f

However, in order to implement this we need a bit of cleverness:

- for the purposes of files, the dot represents a dot;
- for the purposes of namespaces, we must create several namespaces
  (e.g. tack::core).

This is also inline with the idea that the model name does not always
contribute to the namespaces as required by primitives. We basically
need a cleverer version of qname to handle all of these scenarios.

It may also be worth taking into account the other story on this topic
where we considered using underscores instead of folders for facet
names. It may be nicer to have dots for this,
e.g. =types.my_class.hpp=.

Idea:

=qnames= should have a model name and a model package; only the model
package contributes to the namespaces. The model name is unpacked into
multiple model packages (e.g. "a.b" => a::b). The file name uses the
model name, not the model package.

*** Use dots in data files extensions                                 :story:

At the moment we use extensions such as =xmlyarn=. It should really be
=.xml.yarn= or something of the kind.

*** Refactor ownership hierarchy                                      :story:

Start implementing the archetype logic. Basically there is a artefact
unique identifier

- rename it to =artefact_descriptor=.
- remove all dia fields; these are now file importer specific and
  never reach dynamic.
- add =kernel= field. This is set to =stitch= or =quilt=.
- rename formatter field to =kind=

Merged stories:

*Consider adding "application" to ownership hierarchy*

Not all fields make sense to all tools in the dogen suite; some are
knit specific, some are stitch specific and some are shared. At
present this is not a problem because stitch loads up all of knit's
fields and assumes users won't make use of them. If they do, nothing
bad "should" happen. But a better way to solve this may be to only
load fields that belong to an application. We could add "application"
to ownership hierarchy, and filter on that. Note though that we would
need some way of saying "all applications" (e.g. at present, leave the
field blank).

*Consider renaming =ownership_hierarchy=*

We came up with the name =ownership_hierarchy= because we could not
think of anything else. However, it is not a particularly good name,
and it is increasingly so now that we need to use it across models. We
need a better name for this value type.

This work must be integrated with the [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_69.org#thoughts-on-cpp-refactoring][archetype work]].

*Split knitting from stitching settings*

*Rationale*: with "kernel" we will have quilt and stitch.

At present we only have a single common directory with all of the
available fields. Not all fields apply to both stitching and
knitting - but some do. We need a way to filter these. One possibility
is to use an approach similar to the formatter groups in the ownership
hierarchy. For now we simply have fields that have no meaning in
stitching but can be supplied by users.

*** Identifier parser has hard-coded primitives                       :story:

Instead of using the hardware model, we have hard-coded all of the
primitives. In addition, there are some primitives which are C++
specific (=wchar_t=), as well as others which are only valid in
certain cases such as =void=. This needs a bit of thinking.

*** Split formatter properties and associated classes from formattables :story:

We have two kinds of data: the formattables themselves (mapped from
tack) and associated data (formatter properties). The latter is
totally independent. We should create a namespace for all of these
classes and a workflow that produces the data ready for consumption. A
tentative name is =manifest=.

*** Compile dogen in Windows using Visual Studio 2015                 :story:

Using our "SoC" resources, we need to setup a Dogen development
environment on Windows using VS 2015. We need to also create a blog
post about it.

Issues:

- is polymorphic in instrinsics for microsoft, remove comment. see
  patch in github.
- update find boost with MSVC version
- add string to enum io
- update exception classes: remove default in base constructor, and
  add explicit to base and derived as well as by ref.

File with instructions:

0. cd c:\DEVELOPEMENT\output
1. (only once - as admin) update version of msvc in cmake C:\Program
  Files (x86)\CMake\share\cmake-3.3\Modules\FindBoost.cmake
  look for msvc-140 and update it to msvc-150
2. set CMAKE_INCLUDE_PATH=C:\boost\include;C:\DEVELOPEMENT\libxml2-2.7.8.win32\include
   set CMAKE_LIBRARY_PATH=C:\boost\lib;C:\DEVELOPEMENT\libxml2-2.7.8.win32\lib
3. cmake ..\dogen -G "Visual Studio 14 2015" -Wno-dev (CONFIGURATION COMMAND)

if you need to re-run: delete the cache:

del CMakeCache.txt

4. msbuild dogen.sln /t:config

5.msbuild dogen.sln /t:dia /fileLogger   => used to create log for
  errors- called msbuild.log in output directory

Links:

- [[http://dominoc925.blogspot.co.uk/2013/04/how-i-build-boost-for-64-bit-windows.html][How I build Boost for 64 bit Windows]]
- [[https://svn.boost.org/trac/boost/ticket/11449][C++11 - is_polymorphic doesn't work with final-ed class in MSVC.]]
- [[https://github.com/boostorg/type_traits/blob/04a8a9ecc2b02b7334a4b3f0459a5f62b855cc68/include/boost/type_traits/intrinsics.hpp][type_traits/include/boost/type_traits/intrinsics.hpp]]
- [[http://stackoverflow.com/questions/20800166/cmake-compile-with-mt-instead-of-md][CMake - compile with /MT instead of /MD]]
- [[http://www.cmake.org/cmake/help/v3.1/manual/cmake-generators.7.html][CMake Generators]]
- [[http://choorucode.com/2014/06/06/how-to-build-boost-for-visual-studio-2013/][How to build Boost for Visual Studio 2013]]

*** Consider renaming includers                                       :story:

Its very confusing to have header files that include lots of other
header files called "includers". There is too much overloading. We
should consider calling them "master header files" as per Schaling
terminology in the [[http://theboostcpplibraries.com/boost.spirit][boost book]].

*** Replace qname with id's in tack                                   :story:

We don't really need qname in it's current form for the purposes of
tack. We should:

- create a base class for all types in model called element.
- add a property called id to element. Compute id on the basis of
  hashing name and location. Change all model containers,
  relationships etc to use id instead of qname.

*** Rename types in =tack= using MOF/eCore terms                      :story:

Rename the types in =tack= to make them a bit more inline with
MOF/eCore. As much as possible but without going overboard. Ensure we
do not pick up meta-meta-model concepts by mistake. Rename nested
qname to something more sensible from MOF/eCore. Review all concept
names in this light.

*** Create a blog post on biicode                                     :story:

Investigate adding biicode support since we need to add a RapidJson
dependency. Create a blog post about it.

Post has [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/blog/biicode.org][already been started]].

*** Add support for pulling dependencies from biicode                 :story:

[[https://www.biicode.com/][Biicode]] is a nuget-like repo for c++. We should look into both
consuming dependencies from it and pushing dogen into it. In addition
there are associated emblems:

https://github.com/Manu343726/snail

We should also look into [[https://www.biicode.com/biicode-open-source-challenge][the challenge]].

We should push both the C++ libraries as well as the dogen binary.

We should take the least intrusive possible approach to start with, by
creating a split setup for biicode.

*** Update copyright notices                                          :story:

We need to update all notices to reflect personal ownership until DDC
was formed, and then ownership by DDC.

*** Create a set of definitions for tagging and meta-data             :story:

We still use these terms frequently. We should define them in dynamic
to have specific meanings.

*** Models should have an associated language                          :epic:

#+begin_quote
*Story*: As a dogen user, I want to make sure I only use valid system
models so that I don't generate models that code generate but do not
compile.
#+end_quote

Certain models (e.g. system / library models) can only be used in a
give language; for example =boost= and =std= only make sense in C++. A
.Net library model would only make sense in .Net, etc. These are
Language Specific Models (LSM). Once a model depends on a LSM it
itself becomes an LSM and it should not be able to then make use of
models of other languages nor should one be able to request a code
generation for other languages.

However, one day we will have a system model which is a Language
Agnostic Model (LAM). The system model will provide a base set of
functionality across languages such as containers, and for each type
it will have mappings to language specific types. The mapping is
declared as dynamic extensions in the appropriate section
(i.e. =tags::cpp::mapped_type= or something of that ilk). If a model
depends only on LAMs, it is itself a LAM and can be used to generate
code on any supported language (presumably a supported language is
defined to be that for which we have both mappings and a code
generation backend).

A first step for this would be to have a language enumeration in SML
which is a property of the model, and one entry of which is "language
agnostic".

*** Set enumeration underlying type in SML                            :story:

In cpp transformer we have hacked the underlying type of the
enumeration. Remove this hack and set it in SML. Still a hack, but
a tad better.

Actually this could be the first case where LAM/PIM is used: we could
call this something like integer.

This is also hacked in tack_dia's transformer.

*** Add support for Language Agnostic Models (LAM)                    :story:

When we start supporting more than one language, one interesting
feature would be to be able to define a model once and have it
generated for all supported languages. This would be achieved by
having a system model (or set of system models) that define all the
key types in a language agnostic manner. For example:

: lam::string
: lam::int
: lam::int16

Each of these types then has a set of meta-data fields that map them
to a type in a supported language:

: lam:string: cpp.concrete_type_mapping = std::string
: lam:string: csharp.concrete_type_mapping = string

And so on. We load the user model that makes use of LAM, we generate
the merged model still with LAM types and then we perform a
translation for each of the supported and enabled languages: for every
LAM type, we replace all its references with the corresponding
concrete type. We need to split the supplied mapping into a QName, use
the QName to load the system models for that language, look up the
type and replace it. After the translation no LAM types are left. We
end up with N SML merged models where N is the number of supported and
enabled languages.

Each of these models is then sent down to code generation. This should
be equivalent to manually generating models per language - we could
use this as a test.

Once we have LAM, it would be great to be able to exchange data
between languages. This could be done as follows:

- XML: create a "LAM" XML schema, and a set of formatters that read
  and write from it. This is kind of like reverse mapping the types
  back to LAM types when writing the XML.
- JSON: similar approach to XML, minus the schema.
- POF: use the coherence libraries to dump the models into POF.

FIXME: we believed this story was already backloged but could not find
it on a quick search. Do a more thorough search.

*** Thoughts on simplifying the formattables generation               :story:

We have a problem in the way which we are doing the formattables:
because we are doing model traversals for each of the factories, we
cannot easily introduce a set of manually generated qnames such as the
registrar and includers. However, if we started off the main workflow
by creating a structure like so:

- qname
- optional entity (new base class in SML); if null we need to create
  extensions as an empty object.

We then need a list of these that get passed in to all repository
factories. These use a visitor of entity to resolve to a type (where
required).

We can inject types to this list that have a qname but no entity. For
these we generate some parts of the formatter properties. Actually, we
still need to generate inclusion lists even when there is no
entity. Perhaps we need to create a new method in the provider that
does not take an SML entity but still generates the inclusion list.

Actually this should all be done in SML. We should have zero qname
look-ups coming out of SML, just follow references. This story is a
variation of the split between "partial" models and "full" models.

Well not everything should be done in SML. We still need to create a
structure with the properties above, but that is done by iterating
through a list in the SML model.

One slight problem with this approach: sometimes we need to preserve
some relationships in the newly generated objects. For registrar we
need to preserve the model leaves. For the includers / master headers
we need to express somehow the inclusion relationship at the formatter
level. The latter is definitely a special case because it is a pure
C++ concept: include files cannot be modeled in SML. However,
registrar is slightly different because we still need to compute the
includes based on the leaves. This means that the above approach will
not provide a clean solution, unless we synthesise an SML object when
providing the includes. And of course we need to be careful taking
that route or else we will end up generating the object across all
facets.

*** Consider reducing the number of qname lookups in cpp model        :story:

At present we are using qnames all over the place in CPP. Nothing
stops us from using strings instead of qnames if that is more
efficient.

What is worse is that we seem to be doing a ridiculous amount of qname
lookups. It would be much nicer if we could somehow have all the data
in the right shape to avoid doing so many lookups.

This should be done as part of the move to =yarn=.

*** Handling of managed directories is incorrect                      :story:

At present we are querying the tack dia importer to figure out what
the managed directories are. These are basically the top-level
directories from where we want the housekeeper to operate. In reality
this is (or can be placed) in the meta-data. We should be able to
extract the managed directories from the meta-data as a step in one of
the workflows.

This can be done by the backend. It does mean that we should be
returning a composite type from generation:

- list of files;
- list of managed directories.

Alternatively we could have a =managed_directories= method that takes
in an SML model and then internally reads in the meta-data for a given
model to produce the list.

*Merged with previous story*

Compute managed directories from knitting options

At present the backend is returning empty managed directories. This
means housekeeping will fail in the new world. We need to change the
interface of this method to take in the knitting options and return
the managed directories.

This is not entirely trivial. At present the managed directories are
computed in the locator. It takes into account split project, etc to
come up with all the directories used by the backend. We need to make
these decisions during path expansion, expect we only need manged
directories for the root object. However we do not know which object
is the root object at present, during the expansion. We could identify
it via the QName and the SML model in context thought. We could then
populate the managed directories as a text collection. We then need
some settings and a factory to pull out the managed directories from
the root object. This could be done in =managed_directories=, by
having an SML model as input.

*** Add include providers for all types                               :story:

We need to implement the provider container support for primitives,
modules and concepts.

Update:

- inclusion dependencies factory
- provider container

*** Implement all formatter interfaces                                :story:

We still have a couple of skeleton interfaces:

- primitve
- concepts

*** Do not compute inclusion directives for system models             :story:

It seems we are computing inclusion directives and other path
derivatives for system models:

: {
:   "__type__": "dogen::cpp::expansion::path_derivatives",
:   "file_path": "/home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.5/stage/bin/../test_data/all_primitives/actual/std/include/std/serialization/unique_ptr_fwd_ser.hpp",
:   "header_guard": "STD_SERIALIZATION_UNIQUE_PTR_FWD_SER_HPP",
:   "inclusion_directive": "<quote>std/serialization/unique_ptr_fwd_ser.hpp<quote>"
: }

This comes out of the workflow, so we possibly are then ignoring it
for the non-target types. So:

- can we avoid computing these altogether?
- are we ignoring it?

Actually this is the usual problem with the "origin" of the type. We
need a way to determine if this type needs computations or not. We
need to create a story to clean up the =origin_type= and
=generation_type= and then we can make use of it to determine if we
need to compute inclusion, path etc or not.

*** Header guard in formatters should be optional                     :story:

At present we are relying on empty header guards to determine what to
do in boilerplate. We should use boost optional.

*** Remove complete name and use qualified name                       :story:

At present we have both complete name and qualified name in
formatables. Qualified name is blank. We should remove complete name
and populate qualified name.

This is in nested type info.

*** Consider renaming registrar in boost serialisation                :story:

At present we have a registrar formatter that does the boost
serialisation work. However, the name =registrar= is a bit too
generic; we may for example add formatters for static registrars. We
should rename this formatter to something more meaningful. Also the
name registrar is already well understood to mean static registrar.

This is a big problem now that we cannot add a type with the name
registrar to the main model as it clashes with the serialisation
registrar.

We could simply name it serialisation registrar or some such name that
is very unlikely to clash. We should then have a validation rule that
stops users from defining types with that name.

We need to go through all of the renamed registrars and fix them.

*** Create more "utility" members in class info                       :story:

One way of making the templates a bit more manageable is to avoid
having really complex conditions. We could simplify these by giving
them intelligible names and making them properties of the
formattables - mainly class info as that's where the complexity seems
to stem from. For example:

: if ((!c.all_properties().empty() || c.is_parent()) && !c.is_immutable()) {

could be replaced with =has_swap=, or perhaps even =has_public_swap= /
=has_protected_swap=.

*** Consider renaming module path to internal module path             :story:

Since we have got a external module path, it would make sense for the
other to be the internal module path. This may be taking the symmetry
too far, so we need to have a think.

** Deprecated
*** CANCELLED Create knitter options for each frontend                :story:
    CLOSED: [2015-08-05 Wed 17:14]

*Rationale*: not required after latest refactor.

At present some knitting options are specific to a frontend
(particularly in troubleshooting). We should create different classes
to represent options on a per fronend basis.
*** CANCELLED Add identity management to =sml::property_indexer=      :story:
    CLOSED: [2015-08-07 Fri 15:40]

*Rationale*: identity was removed.

At present we are populating the identity properties in dia to sml. We
need to move this to property indexer in SML.

We found a problem with moving this: we need the identity properties
to be in the object before we inject system types (they are used to
generate keys) but property indexing happens after injection. We
cannot move property indexing to be before injection (we need system
types to exist). We probably need to split property indexing into
two. The other problem is that if we take into account concepts, the
identity properties should only be indexing after concepts have been
indexed. This requires a bit of thinking.

See [[https://github.com/DomainDrivenConsulting/dogen/blob/master/patches/move_identity_attribute_to_sml.patch][the patch]] for the latest on this.
*** CANCELLED Support ordering of includes                            :story:
    CLOSED: [2015-08-07 Fri 16:57]

*Rationale*: we already have a function to order includes more or less
along these lines. There is no use case for further configurability.

One of my personal preferences has always been to group includes by
"library". Normally first come the C includes, then the standard
library ones, then boost, then utilities and finally types of the same
model. Each of these can be thought of as a group. Inside each group
the file names are normally ordered by size, smallest first. It would
be nice to have support for such a feature in Dogen.

Formatters would then push their includes into the correct
group. Group names could be the model name (=std=, etc).

A bit of a nitpick but nice nonetheless.

*** CANCELLED Consider renaming =knit= to =weave=                     :story:
    CLOSED: [2015-08-07 Fri 17:03]

*Rationale*: term reserved for AOP. Added story to keep track of terms.

We seem to have missed an obvious term: weaving. We can either save it
for later or perhaps rename =knit= to =weave=.

Actually, since weave is a well-known term in AOP, we should save it
for if/when we decide to support AOP.

*** CANCELLED Adding a dependency to a non-existent expander crashes dogen :story:
    CLOSED: [2015-08-07 Fri 17:17]

*Rationale*: expanders have long since been removed.

We are not checking that all dependencies exist when building the
graph. If we add a dependency to a expander that does not exist we
crash and burn:

: /home/marco/Development/DomainDrivenConsulting/dogen/projects/knit/spec/workflow_spec.cpp(550): last checkpoint
: dogen_knit_spec: /usr/include/boost/smart_ptr/shared_ptr.hpp:653: typename boost::detail::sp_member_access<T>::type boost::shared_ptr<dogen::dynamic::expansion::expander_interface>::operator->() const [Y = dogen::dynamic::expansion::expander_interface]: Assertion `px != 0' failed.
: unknown location(0): fatal error in "all_primitives_model_generates_expected_code": signal: SIGABRT (application abort requested)

The cause of this is that we may end up creating vertices for
dependencies (initialised with a null shared pointer) but never
actually =add= the expander that corresponds to that expander name to
the graph. We then visit the graph and assume all vertices have valid
expanders, which results in the error above.

We can do two things:

- validate that all dependencies exist by placing all expanders in a
  set and resolving the dependencies; this can be done before the
  graph.
- checking that the expander pointer points to not null or throw.

*** CANCELLED Consider introducing =archetypes= to simplify output models :story:
    CLOSED: [2015-08-07 Fri 17:38]

*Rationale*: this story is far too complicated and confusing. We need
to continue thinking around this area (and take some ideas from this)
but the entire thing is unusable.

We haven't quite arrived at the ideal configuration for the cpp
model. We are close, but not there yet. The problem we have at the
moment is that the formatters drive a lot of the work in
formattables, resulting in a circular dependency. This is happening
because we are missing some entities. This story is just a random set
of thoughts in this space, trying to clear up the terminology across
the board.

*Random thoughts*

What is probably needed is to have facets, aspects and "file kinds" as
top-level concepts rather than just strings with which we label
formatters. In addition, we need a good name for "file kinds". This is
a meta-concept, something akin to a file template. The formatter
produces a physical representation of that meta-concept. As part of
the formatter registration, we can also register this meta-concept
(provided it relies on an existing formattable). And in effect, these
are the pieces of the puzzle:

- you define a "file kind".
- a facet and a model are groupings of "file kinds". These happen to
  be hierarchical groupings. There are others: header and
  implementation, or class header formatter. Those are
  non-hierarchical.
- you bind a transformer to a SML type to generate a formattable.
- a formattable is associated with one or more "file kinds" or better
  yet a file kind is associated with a formattable. It is also
  associated with formatting properties and settings. It is those
  tuples that we pass to the formatters.
- you bind a formatter to a "file" and process the associated
  formattable.

Perhaps we can call these "file kinds" file archetypes or just
archetypes.

What can be said about an archetype:

- conceptual notion of something we want to generate.
- one SML entity can map to zero or many archetypes. Concept at
  present maps to zero. Object maps to many.
- a representation of the archetype as source code is done by the
  formatter. It uses a template to help it generate that
  representation.
- a given archetype maps to one and only one SML entity.
- a given archetype maps to one and only one CPP entity.
- archetypes can be grouped in many ways. One way is facets and
  models.
- archetypes have definitions: name of the archetype, what groups it
  belongs to.
- archetypes have associated data: formattables, settings,
  properties. This is an entity and needs a name.
- formatters work on one and only one archetype.
- archetypes have qualified names; this is (mostly) what we called
  ownership hierarchy. Qualified names can be represented as separate
  fields or using the dot notation.
- archetypes have labels: this is what we called groups.
- dynamic is a model designed to augment SML with some archetype
  data. This is not true in the dia case. Check all fields to see if
  it is true everywhere else.
- an aspect is a property of one or more archetypes; it is a knob that
  affects the generation of the source code representation.
- an archetype instance belongs to an archetype.
- we should remove the concept of "integrated facets". It just happens
  that a facet such as types may have aspects that enable features
  similar to aspects in other facets. There may be rules that
  determine that when certain aspects are enabled, certain facets must
  be switched off because they are incompatible.
- facet is a good name for grouping archetypes, but model isn't. We
  need a better name for a set of facets. Aspect is also a good
  name. In addition, a model group is also a bad name. A "model" is a
  cohesive group of archetypes that are meant to be used together. A
  "model group" is a cohesive group of models that provide the same
  conceptual representations in different programming languages. Maybe
  we should use a more "random" name such as: pod. Then perhaps a
  model group could become a "pod family": a family of related pods. A
  given model can be represented by one pod family or another - they
  are mutually exclusive. Of course, from a command line perspective,
  its better to think of "modes". Each mode corresponds to choosing
  one "pod family" over another. This does not map very cleanly.
- archetypes have an associated programming language - a grammar.
- a facet may exist in more than one programming language and an
  aspect too.
- pods are programming language specific.
- formattables are kind of like an archetype friendly representation
  of the domain types. We need a good name for this.
- internal and external now make slightly more sense, at least once we
  got a good name for formatters. We still need a good name for it
  though. If the archetype instance is generated because of the
  presence of the domain type, it is external. If the archetype has no
  sensitivity to domain types (but may have sensitivity to other
  things such as options) it is internal. The naming around this is
  not totally clear.
- internal formatters may not be allowed to be disabled. For example,
  if serialisation is on, registrar must be generated. With
  CMakeLists, we may want do disable them altogether.
- in the thrift story in the backlog we mention the existence of
  mutually exclusive groups of facets. We should also come up with a
  name for these.
- archetype may not quite be the right name. See [[http://www.pearsonhighered.com/samplechapter/032111230X.pdf][Archetypes and
  archetype patterns]]. See also:
  - [[http://www.step-10.com/SoftwareDesign/ModellingInColour/ColourCoding.html][Class Archetypes, UML and Colour]]
  - [[http://www.step-10.com/SoftwareDesign/ModellingInColour/index.html][Peter Coad's 'Modeling in Color']]
  - [[http://www.step-10.com/Books/JMCUBook.html][Java Modeling in Color with UML]]
- the process of mapping domain types to archetypes could be called
  "expansion" because its a one to many relationship in most cases.
- its not quite correct to call CPP types "formattables". The
  archetype has to have an ordered container of inputs to the
  formatter. This is sort of the "payload" for formatting; the
  archetype is a container of such entities. Taking into account the
  cases where more than one type is placed in the same file, this
  would result in the includes being merged. Or perhaps these things
  are really formattables, but then we need a way to distinguish
  between "top-level formatters" that generate archetypes from
  "partial" formatters that can be combined.
- with "facet specific types" we go one level deeper: it should be
  possible to add an enumeration definition to say test data. This
  would mean that archetypes and facets are not quite so aligned as we
  first thought. Potentially, one should be able to ask for say a
  formattable at facet X in an artchetype at facet Y.
- One way to look at it is as follows: there is the modeling
  dimension, in which we have an entity, say entity =A=; and there is
  the implementation dimension, in which =a= can be represented by
  =A1, A2, ..., An= archetypes. In effect, the implementation
  dimension has multiple dimensions, one for each pod (and of course
  the pod families would be an extra dimension and so on). Actually,
  we probably have 3 steps: the modeling dimension, the translation of
  that into a language-specific representation and then finally the
  archetype dimension.
- a good name for the top-level container of archetypes is
  "kernel". This was inspired (loosely) in some ideas from EMF. So
  we'd have say the "quilt kernel", with support for multiple
  programming languages such as cpp, java etc. We we'd have the "pleat
  kernel" and so forth. Each kernel has a set of languages and the
  languages have archetypes. Archetypes have a collection of
  properties such as the formattables they need, the formatters and so
  on. The job of a model such as =quilt::cpp= is to implement this
  binding.
- dynamic fields can be owned by archetypes or by other types of
  owners (e.g. dia). We should have a way of expressing this
  ownership.
- we haven't used the word "feature" anywhere yet (properly; we
  mentioned it in the manual and so on, but not given it any good
  meaning).
- we created a split between "internal" and "external" formatters, but
  its interesting to notice that we have "internal" formatters that
  are "regular" formatters - in that we need to create a qname for
  them and the formatter properties will work correctly; whereas some
  others are "irregular" formatters - they have strange filenames that
  cannot be generated without some fiddling. Actually, ODB options is
  the main problematic one. If we could place it in a sensible
  location we could probably get rid of irregular formatters
  altogether.
- we need to have "special" facets; cmake files for example should not
  really have a facet but it seems having an empty facet name breaks a
  lot of stuff.
- we need a map between types/states in SML and enablement. For
  example, if a type is "non-generatable" that is taken to mean
  "generate types if file does not exist, default all else to
  disabled". We need a way to express this sort of logic. This is akin
  to an "enablement map". For example, users could define these maps
  somewhere, given them a name and then assign a type to a map. In
  addition, we need a way to express "generate but don't override" and
  "generate and override".

*Merged with other stories*

It is important not to confuse formatters with archetypes. A formatter
(or at least, a "top-level formatter"; those that generate files) is
in a sense a "category" of archetypes. In other words, for a given
formatter many archetypes will be generated. This may mean that the
"archetype" is not a very good choice because it may imply some kind
of meta-class-ness. In a sense, we are dealing with arch-entities
("entity" being SML's base class for all modeled domain types). So
fundamentally, the correct workflow is vaguely like this:

- we create a model for some problem domain. We represent this model
  in SML. All objects are identifiable by a qname.
- we apply a transformation of this model into something which is
  closer to the programming language that we wish to generate; these
  we choose to call formattables.
- we may also inject some formattables which do not have a mapping to
  the original domain objects. These have synthetic qnames.
- we apply a function that takes the qname, the SML entity, the
  formattable and generates an archetype skeleton. To start off with,
  this is made up of only a file name and a top-level formatter. The
  structure exists in memory as a map of qnames to formatter names to
  archetypes.
- we then fill in the blanks: compute includes, enablement, etc. The
  final blank that needs to be filled in is the generation of the
  file, which is done by applying a formatter to a number of the
  archetype properties.

Another point of interest is that we may be able to move some of the
archetype processing to common code. For example, file name
generation, enablement, and so on are not language specific. However,
we need to have a representation of the archetype which is specific to
a model (e.g. =quilt::cpp= say) because not all properties will be
common. We could, possibly, have an archetype base class, which then
would imply a formatter's base class and so on - but then we hit the
visitor across models problem.

In this approach we do have an advantage which is we can parallelise a
lot of work across each stage in the "pipeline". For instance we can
run transformation from SML to formattables in parallel. We could
conceivably even have futures for each of the archetype
properties. None of this is a concern for the foreseable future, of
course.

FIXME: improve references by having models inside of models; we should
be able to keep only the types that we refer in the final model.
